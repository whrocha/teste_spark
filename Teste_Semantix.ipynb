{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste Apache Spark - Willian Henrique Barbosa Rocha\n",
    "<hr>\n",
    "\n",
    "## Qual o objetivo do comando cache em Spark?\n",
    "O comando cache tem como objetivo colocar os data-sets in memory atraves dos cluster, para que o acesso a esses dados fiquem mais rapidos sem a necessidade de ler novamente em disco. Caso nao seja possivel alocar o data-set na memoria,o Spark ira fazer o split desses dados no disco.\n",
    "\n",
    "## O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?\n",
    "O Spark foi concebido para trabalhar com os dados in-memory, ou seja, utilizando o acesso a memoria, enquanto o Hadoop utiliza-se do acesso ao disco para processar as consultas aos dados. Porem o hardware para processar Spark precisa ser mais robusto, devido ao maior utilizacao de memoria.\n",
    "\n",
    "## Qual é a função do *SparkContext* ?\n",
    "O SparkContext representa a conexao, um objeto do cluster Spark, e pode ser utilizado para criar os RDDs, acumuladores e variáveis de broadcast no cluster Spark.\n",
    "\n",
    "## Explique com suas palavras o  que é Resilient Distributed Datasets (RDD).\n",
    "RDD's são as estruturas de dados fundamentais do Spark, distribuições imutáveis e tolerantes a falhas das coleções de objetos que são paralelizados através do cluster. Existem 2 tipos de operações: Transformations, Actions.\n",
    "\n",
    "## *GroupByKey* é menos eficiente que *reduceByKey* em grandes dataset. Por quê?\n",
    "O comando **GroupByKey** realiza o Suffle dos dados através da rede para formar a lista (K, V), isso é particularmente custoso quando estamos trabalhando com um conjunto de dados de grande volume, entretanto o comando **reduceByKey** realiza o suffle dos dados somente nos resultados da operação reduce, isso resultado em significante redução de tráfico na rede.\n",
    "\n",
    "## Explique o  que o  código Scala abaixo faz.\n",
    "```scala\n",
    "val textFile = sc.textFile(\"hdfs://...\")\n",
    "val counts   = textFile.flatMap(line => line.split(\"\"))\n",
    "               .map(word => (word,1))\n",
    "               .reduceByKey(_ + _)\n",
    "counts.saveAsTextFile(\"hdfs://...\")\n",
    "```\n",
    "\n",
    "Codigo de word count, realiza o mapping das palavras, e a realiza a contagem de quantas vezes aquela palavra apareceu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP requests to the NASA Kennedy Space Center WWW server\n",
    "<hr>\n",
    "\n",
    "**Fonte oficial do dateset:** [http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)\n",
    "\n",
    "**Dados:**\n",
    "- [Jul​ ​ 01​ ​ to​ ​ Jul​ ​ 31,​ ​ ASCII​ ​ format,​ ​ 20.7​ ​ MB​ ​ gzip​ ​ compressed](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz)​ , ​ ​ 205.2​ ​ MB.\n",
    "- [Aug​ ​ 04​ ​ to​ ​ Aug​ ​ 31,​ ​ ASCII​ ​ format,​ ​ 21.8​ ​ MB​ ​ gzip​ ​ compressed](ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz)​ , ​ ​ 167.8​ ​ MB.\n",
    "\n",
    "**Sobre o dataset**​ : Esses dois conjuntos de dados possuem todas as requisições HTTP para o servidor da NASA Kennedy\n",
    "Space​ ​ Center​ ​ WWW​ ​ na​ ​ Flórida​ ​ para​ ​ um​ ​ período​ ​ específico.\n",
    "\n",
    "Os​ ​ logs​ ​ estão​ ​ em​ ​ arquivos​ ​ ASCII​ ​ com​ ​ uma​ ​ linha​ ​ por​ ​ requisição​ ​ com​ ​ as​ ​ seguintes​ ​ colunas:\n",
    "- **Host fazendo a requisição**​ . Um hostname quando possível, caso contrário o endereço de internet se o nome\n",
    "não​ ​ puder​ ​ ser​ ​ identificado.\n",
    "- **Timestamp**​ ​ no​ ​ formato​ ​ \"DIA/MÊS/ANO:HH:MM:SS​ ​ TIMEZONE\"\n",
    "- **Requisição​ ​ (entre​ ​ aspas)**\n",
    "- **Código​ ​ do​ ​ retorno​ ​ HTTP**\n",
    "- **Total​ ​ de​ ​ bytes​ ​ retornados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "Fontes de consultas utilizadas\n",
    "\n",
    "- [Manual de referência do SparkSQL](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [exemplo regex](https://stackoverflow.com/questions/53090003/split-string-in-a-spark-dataframe-column-by-regular-expressions-capturing-groups)\n",
    "- [exemplo de split column](https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando a sessao spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# importando as lib que serao utilizadas para fazer a limpeza dos dados\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import trim\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Teste Técnico de Apache Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean_log_file(file_name):\n",
    "    # https://spark.apache.org/docs/2.2.1/sql-programming-guide.html#generic-loadsave-functions\n",
    "    df_temp = spark.sql(\"SELECT * FROM csv.`\" + file_name + \"`\")\n",
    "    \n",
    "    df_temp = df_temp.select(\"_c0\",\n",
    "                             f.split(\"_c0\", \" - - \")[0].alias(\"host_requisicao\"),\n",
    "                             f.split(\"_c0\", \" - - \")[1].alias(\"_c1\"))\n",
    "    \n",
    "    df_temp = df_temp.select(\"host_requisicao\",\n",
    "                             f.split(f.regexp_replace(\"_c1\",\"\\[([^]]+)\\]\",r\"$1,\"),\",\")[0]\n",
    "                             .alias(\"timestamp_requisicao\"),\n",
    "                             f.split(f.regexp_replace(\"_c1\",\"\\[([^]]+)\\]\",r\"$1,\"),\",\")[1]\n",
    "                             .alias(\"_c2\"))\n",
    "    \n",
    "    df_temp = df_temp.select(\"host_requisicao\",\"timestamp_requisicao\",\n",
    "                             trim(f.split(f.regexp_replace(\"_c2\",\"\\\"([^]]+)\\\"\",r\"$1,\"),\",\")[0])\n",
    "                             .alias(\"requisicao\"),\n",
    "                             trim(f.split(f.regexp_replace(\"_c2\",\"\\\"([^]]+)\\\"\",r\"$1,\"),\",\")[1])\n",
    "                             .alias(\"_c3\"))\n",
    "    \n",
    "    df_temp = df_temp.select(\"host_requisicao\",\"timestamp_requisicao\",\"requisicao\",\n",
    "                             (f.split(\"_c3\", \" \")[0]).alias(\"codigo_retorno_http\"),\n",
    "                             regexp_replace(f.split(\"_c3\", \" \")[1],\"-\",\"0\").alias(\"bytes_retorno\"))\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jul = data_clean_log_file(\"access_log_Jul95\")\n",
    "df_ago = data_clean_log_file(\"access_log_Aug95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+-------------+\n",
      "|     host_requisicao|timestamp_requisicao|          requisicao|codigo_retorno_http|bytes_retorno|\n",
      "+--------------------+--------------------+--------------------+-------------------+-------------+\n",
      "|   in24.inetnebr.com|01/Aug/1995:00:00...|GET /shuttle/miss...|                200|         1839|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|      GET / HTTP/1.0|                304|            0|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|GET /images/ksclo...|                304|            0|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|GET /images/MOSAI...|                304|            0|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|GET /images/USA-l...|                304|            0|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995:00:00...|GET /images/launc...|                200|         1713|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|GET /images/WORLD...|                304|            0|\n",
      "|slppp6.intermind.net|01/Aug/1995:00:00...|GET /history/skyl...|                200|         1687|\n",
      "|piweba4y.prodigy.com|01/Aug/1995:00:00...|GET /images/launc...|                200|        11853|\n",
      "|slppp6.intermind.net|01/Aug/1995:00:00...|GET /history/skyl...|                200|         9202|\n",
      "|slppp6.intermind.net|01/Aug/1995:00:00...|GET /images/ksclo...|                200|         3635|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995:00:00...|GET /history/apol...|                200|         1173|\n",
      "|slppp6.intermind.net|01/Aug/1995:00:00...|GET /history/apol...|                200|         3047|\n",
      "|     uplherc.upl.com|01/Aug/1995:00:00...|GET /images/NASA-...|                304|            0|\n",
      "|        133.43.96.45|01/Aug/1995:00:00...|GET /shuttle/miss...|                200|        10566|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995:00:00...|      GET / HTTP/1.0|                200|         7280|\n",
      "|kgtyk4.kj.yamagat...|01/Aug/1995:00:00...|GET /images/ksclo...|                200|         5866|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995:00:00...|GET /history/apol...|                200|         2743|\n",
      "|ix-esc-ca2-07.ix....|01/Aug/1995:00:00...|GET /shuttle/reso...|                200|         6849|\n",
      "|     d0ucr6.fnal.gov|01/Aug/1995:00:00...|GET /history/apol...|                200|        14897|\n",
      "+--------------------+--------------------+--------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ago.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questões\n",
    "Responda​ ​ as​ ​ seguintes​ ​ questões​ ​ devem​ ​ ser​ ​ desenvolvidas​ ​ em​ ​ Spark​ ​ utilizando​ ​ a ​ ​ sua​ ​ linguagem​ ​ de​ ​ preferência.\n",
    "\n",
    "#### 1. Número​ ​ de​ ​ hosts​ ​ únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   QTD|\n",
      "+------+\n",
      "|137979|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_jul.createOrReplaceTempView(\"log_jul\")\n",
    "df_ago.createOrReplaceTempView(\"log_ago\")\n",
    "sql = \"\"\"select count(distinct QTD) as QTD \n",
    "            from (SELECT host_requisicao as QTD \n",
    "                    FROM log_jul\n",
    "                   union all\n",
    "                  SELECT host_requisicao FROM log_ago)\"\"\"\n",
    "sqlDF = spark.sql(sql)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. O​ ​ total​ ​ de​ ​ erros​ ​ 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  QTD|\n",
      "+-----+\n",
      "|20847|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"select count(*) as QTD\n",
    "            from (SELECT *\n",
    "                    FROM log_jul\n",
    "                   union all\n",
    "                  SELECT * FROM log_ago)\n",
    "         where codigo_retorno_http = '404'\n",
    "       \"\"\"\n",
    "sqlDF = spark.sql(sql)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Os​ ​ 5 ​ ​ URLs​ ​ que​ ​ mais​ ​ causaram​ ​ erro​ ​ 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|QTD|     host_requisicao|\n",
      "+---+--------------------+\n",
      "|251|hoohoo.ncsa.uiuc.edu|\n",
      "|157|piweba3y.prodigy.com|\n",
      "|132|jbiagioni.npt.nuw...|\n",
      "|114|piweba1y.prodigy.com|\n",
      "| 91|www-d4.proxy.aol.com|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "        select * from (\n",
    "        select count(*) as QTD, host_requisicao\n",
    "            from (SELECT *\n",
    "                    FROM log_jul\n",
    "                   union all\n",
    "                  SELECT * FROM log_ago)\n",
    "         where codigo_retorno_http = '404'\n",
    "         group by host_requisicao) \n",
    "         order by qtd desc \n",
    "         limit 5\n",
    "       \"\"\"\n",
    "sqlDF = spark.sql(sql)\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Quantidade​ ​ de​ ​ erros​ ​ 404​ ​ por​ ​ dia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| QTD|dia|\n",
      "+----+---+\n",
      "| 557| 01|\n",
      "| 291| 02|\n",
      "| 775| 03|\n",
      "| 704| 04|\n",
      "| 732| 05|\n",
      "|1010| 06|\n",
      "|1103| 07|\n",
      "| 688| 08|\n",
      "| 626| 09|\n",
      "| 713| 10|\n",
      "| 734| 11|\n",
      "| 666| 12|\n",
      "| 740| 13|\n",
      "| 698| 14|\n",
      "| 581| 15|\n",
      "| 515| 16|\n",
      "| 673| 17|\n",
      "| 721| 18|\n",
      "| 848| 19|\n",
      "| 730| 20|\n",
      "| 639| 21|\n",
      "| 477| 22|\n",
      "| 577| 23|\n",
      "| 748| 24|\n",
      "| 875| 25|\n",
      "| 702| 26|\n",
      "| 706| 27|\n",
      "| 504| 28|\n",
      "| 420| 29|\n",
      "| 571| 30|\n",
      "| 523| 31|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "        select count(*) as QTD, substr(timestamp_requisicao,1,2) as dia\n",
    "            from (SELECT *\n",
    "                    FROM log_jul\n",
    "                   union all\n",
    "                  SELECT * FROM log_ago)\n",
    "         where codigo_retorno_http = '404'\n",
    "         group by substr(timestamp_requisicao,1,2)\n",
    "         order by dia\n",
    "       \"\"\"\n",
    "sqlDF = spark.sql(sql)\n",
    "sqlDF.show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. O​ ​ total​ ​ de​ ​ bytes​ ​ retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   TOTAL_BYTES|\n",
      "+--------------+\n",
      "|6.551669721E10|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "        select sum(bytes_retorno) as TOTAL_BYTES\n",
    "            from (SELECT *\n",
    "                    FROM log_jul\n",
    "                   union all\n",
    "                  SELECT * FROM log_ago)\n",
    "       \"\"\"\n",
    "sqlDF = spark.sql(sql)\n",
    "sqlDF.show(31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
